services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.3
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - default

  kafka:
    image: confluentinc/cp-kafka:7.5.3
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9094:9094"  
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:9094
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks:
      - default

  spark:
    
    build:
      context: ../            
      dockerfile: docker/spark.Dockerfile
    image: finpipe/spark-py:3.5
    container_name: spark
    user: "0:0"
    command: >
      bash -lc "
      if ! getent passwd 1001 >/dev/null; then
        echo 'sparkuser:x:1001:0:Spark User:/home/sparkuser:/bin/bash' >> /etc/passwd;
      fi;
      mkdir -p /home/sparkuser /data /tmp/.ivy2 /tmp/.m2;
      chown -R 1001:0 /home/sparkuser /data /tmp/.ivy2 /tmp/.m2;
      sleep infinity"
    environment:
      - SPARK_MODE=master
      - SPARK_NO_DAEMONIZE=true
      - HOME=/tmp
      - USER=sparkuser
      - HADOOP_USER_NAME=sparkuser
      - PYSPARK_PYTHON=/usr/bin/python3
      - JAVA_TOOL_OPTIONS=-Duser.name=sparkuser -Duser.home=/tmp
    volumes:
      - ../:/app:rw        
      - ../data:/data:rw   
    working_dir: /app
    depends_on:
      - kafka
    networks:
      - default

networks:
  default:
    driver: bridge