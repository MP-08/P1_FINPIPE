# ğŸš€ FinPipe â€” Data Engineering Pipeline

> **FinPipe** es un proyecto de **Data Engineering end-to-end**, diseÃ±ado para simular el flujo completo de datos financieros en tiempo real.  
> Implementa una arquitectura **medallion (Bronze â†’ Silver â†’ Gold)** utilizando **Apache Kafka**, **Apache Spark Structured Streaming** y **Delta Lake**.

---

## ğŸ§  Objetivo del proyecto

El propÃ³sito de FinPipe es construir un pipeline de datos escalable y modular que permita:
- Ingestar transacciones financieras en tiempo real.
- Procesar, validar y limpiar datos con Spark.
- Persistir datos en formato **Delta Lake**.
- Generar **tablas analÃ­ticas (Gold)** con agregaciones y rankings de usuarios.

Este pipeline emula el trabajo diario de un **Data Engineer** en un entorno productivo.

---

## ğŸ—ºï¸ Roadmap del Proyecto

### ğŸ”„ RelaciÃ³n entre los dos roadmaps: infraestructura + pipeline tÃ©cnico

| Tipo de Roadmap | Etapa | DescripciÃ³n real | Estado actual |
|-----------------|--------|------------------|---------------|
| ğŸ§± **Infraestructura (publicado)** | **Etapa 1 â€“ On-Premise / Local** | ConstrucciÃ³n del entorno local distribuido (Docker, Linux, Kafka, Spark, Delta Lake). Incluye el desarrollo del pipeline y su maduraciÃ³n (Bronze â†’ Silver â†’ Gold). | âœ… En curso |
| | **Etapa 2 â€“ Orquestador (Airflow)** | IntegraciÃ³n de **Apache Airflow** para controlar dependencias y programar los jobs Spark/Kafka. | ğŸ”œ PrÃ³ximo hito |
| | **Etapa 3 â€“ Cloud-Native** | MigraciÃ³n del stack a servicios administrados (**AWS S3 / GCS / Dataproc / MSK**, etc.) con infraestructura como cÃ³digo. | â³ Futuro |
| âš™ï¸ **Pipeline tÃ©cnico (actual)** | **Etapa 1a â€“ Ingesta (Kafka â†’ Bronze)** | Streaming de transacciones y almacenamiento crudo en Delta Lake. | âœ… |
| | **Etapa 1b â€“ TransformaciÃ³n (Bronze â†’ Silver)** | Limpieza, parseo y validaciÃ³n de datos. | âœ… |
| | **Etapa 1c â€“ CuraciÃ³n (Silver â†’ Gold)** | Agregaciones, KPIs y datos listos para analÃ­tica. | âœ… (etapa actual) |

---

## ğŸ—ï¸ Arquitectura general


Cada capa cumple una funciÃ³n clara:
| Capa | Rol principal |
|------|----------------|
| **Bronze** | Ingesta cruda desde Kafka (eventos transaccionales). |
| **Silver** | NormalizaciÃ³n, limpieza y validaciÃ³n de calidad de datos. |
| **Gold** | Agregaciones diarias y ranking de usuarios por volumen. |

---

## âš™ï¸ Stack tecnolÃ³gico

| Componente | DescripciÃ³n |
|-------------|-------------|
| ğŸ **Python 3.10+** | Lenguaje principal. |
| â˜• **Apache Spark 3.5.1** | Motor de procesamiento distribuido. |
| ğŸ§± **Delta Lake 3.2.0** | Formato transaccional ACID sobre archivos parquet. |
| ğŸ”„ **Apache Kafka** | Ingesta de datos en tiempo real. |
| ğŸ³ **Docker Compose** | OrquestaciÃ³n local de servicios. |
| ğŸ§° **Makefile** | AutomatizaciÃ³n de comandos y flujos. |

---

## ğŸ“‚ Estructura del proyecto

| Carpeta / Archivo | DescripciÃ³n |
|--------------------|-------------|
| `jobs/` | Scripts de procesamiento (streaming, transformaciones, gold). |
| `kafka/` | Productor y consumidor Kafka. |
| `scripts/` | Scripts utilitarios y de mantenimiento. |
| `docker/` | ConfiguraciÃ³n Docker Compose. |
| `data/` | Data lake local (excluido del repo). |
| `logs/` | Logs de ejecuciÃ³n. |
| `Makefile` | Orquestador de comandos. |
| `requirements.txt` | Dependencias de Python. |
| `README.md` | DocumentaciÃ³n del proyecto. |

---

## ğŸ§© Flujo de ejecuciÃ³n (modo manual)

Cada etapa se ejecuta en una terminal separada ğŸ‘‡

```bash
# 1ï¸âƒ£ Iniciar Kafka y Zookeeper
make start-docker

# 2ï¸âƒ£ Iniciar el stream Kafka â†’ Bronze
make stream-bronze

# 3ï¸âƒ£ Iniciar el stream Bronze â†’ Silver (Delta)
make bronze-to-silver

# 4ï¸âƒ£ Iniciar el stream Silver â†’ Gold (Delta)
make silver-to-gold

# 5ï¸âƒ£ Largar el productor de transacciones falsas
make run-producer

---

ğŸ’¡ Todos los logs se guardan automÃ¡ticamente en:

logs/dev/
 â”œâ”€â”€ producer.log
 â”œâ”€â”€ bronze.log
 â”œâ”€â”€ silver.log
 â””â”€â”€ gold.log

---

âš™ï¸ Flujo automÃ¡tico (modo tmux)

Si tenÃ©s instalado tmux, podÃ©s correr todo el pipeline en una sola terminal:
make tmux-up

Esto crea una sesiÃ³n con 4 paneles:

Producer

Kafka â†’ Bronze

Bronze â†’ Silver

Silver â†’ Gold

Para salir sin detener nada:
Ctrl + b  luego  d

Y para volver:
tmux attach -t finpipe

---

ğŸ§¹ Limpieza y mantenimiento

Reiniciar entorno de desarrollo:
make reset

Borrar logs antiguos:
make clean-logs

Apagar todo:
make kill-all

---

ğŸ§  Conceptos clave aplicados

âœ… Kafka Topics â†’ transmisiÃ³n de eventos financieros simulados.
âœ… Spark Structured Streaming â†’ lectura en tiempo real con tolerancia a fallas.
âœ… Delta Lake â†’ formato ACID con control de versiones y schema evolution.
âœ… Data Validation Layer â†’ separaciÃ³n automÃ¡tica de datos vÃ¡lidos y rechazados.
âœ… Watermarks & Deduplication â†’ manejo de eventos duplicados o tardÃ­os.
âœ… Aggregation Layer â†’ tablas Gold con mÃ©tricas por fecha, usuario y moneda.
âœ… Makefile Orchestration â†’ ejecuciÃ³n reproducible y controlada del pipeline.

---

## ğŸŒ ConfiguraciÃ³n de entornos

El pipeline soporta mÃºltiples entornos configurables a travÃ©s de la variable `ENV`:

| Variable | DescripciÃ³n |
|-----------|--------------|
| `ENV=dev`  | Modo de desarrollo (por defecto). |
| `ENV=prod` | Modo productivo simulado. |

Ejemplo de ejecuciÃ³n:
```bash
ENV=prod make bronze-to-silver

Los datos se escribirÃ¡n automÃ¡ticamente en la ruta:
data/prod/...


ğŸš€ PrÃ³ximos pasos (Roadmap tÃ©cnico)
Etapa	DescripciÃ³n	Estado
ğŸª„ Etapa 2 â€” Orquestador	IntegraciÃ³n con Apache Airflow o Prefect para manejar dependencias entre jobs.	ğŸ”œ PrÃ³ximo
â˜ï¸ Etapa 3 â€” Cloud Deployment	MigraciÃ³n del stack a AWS (S3 + MSK + EMR) o GCP (GCS + Dataproc + Pub/Sub).	â³ Planificado
ğŸ“Š Etapa 4 â€” VisualizaciÃ³n	CreaciÃ³n de dashboards analÃ­ticos con Tableau, Power BI o Streamlit.	ğŸ”œ Futuro
ğŸ‘¨â€ğŸ’» Autor

MatÃ­as Ezequiel Padilla Presas
ğŸ“ Data Engineer | Arquitecto BIM | Python | SQL | Spark | Data Pipelines & Cloud

ğŸ”— LinkedIn

ğŸ’» GitHub

ğŸ§  â€œFinPipe fue desarrollado con enfoque en la calidad de datos, escalabilidad y buenas prÃ¡cticas de ingenierÃ­a, replicando un entorno productivo real.â€