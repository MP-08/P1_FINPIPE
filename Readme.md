# ğŸš€ FinPipe â€” Data Engineering Pipeline

> **FinPipe** es un proyecto de **Data Engineering end-to-end**, diseÃ±ado para simular el flujo completo de datos financieros en tiempo real.  
> Implementa una arquitectura **medallion (Bronze â†’ Silver â†’ Gold)** utilizando **Apache Kafka**, **Apache Spark Structured Streaming** y **Delta Lake**.

---

## ğŸ§  Objetivo del proyecto

El propÃ³sito de FinPipe es construir un pipeline de datos escalable y modular que permita:
- Ingestar transacciones financieras en tiempo real.
- Procesar, validar y limpiar datos con Spark.
- Persistir datos en formato **Delta Lake**.
- Generar **tablas analÃ­ticas (Gold)** con agregaciones y rankings de usuarios.

Este pipeline emula el trabajo diario de un **Data Engineer** en un entorno productivo.

---

## ğŸ—ºï¸ Roadmap del Proyecto

### ğŸ”„ RelaciÃ³n entre los dos roadmaps: infraestructura + pipeline tÃ©cnico

| Tipo de Roadmap | Etapa | DescripciÃ³n real | Estado actual |
|-----------------|--------|------------------|---------------|
| ğŸ§± **Infraestructura (publicado)** | **Etapa 1 â€“ On-Premise / Local** | ConstrucciÃ³n del entorno local distribuido (Docker, Linux, Kafka, Spark, Delta Lake). Incluye el desarrollo del pipeline y su maduraciÃ³n (Bronze â†’ Silver â†’ Gold). | âœ… En curso |
| | **Etapa 2 â€“ Orquestador (Airflow)** | IntegraciÃ³n de **Apache Airflow** para controlar dependencias y programar los jobs Spark/Kafka. | ğŸ”œ PrÃ³ximo hito |
| | **Etapa 3 â€“ Cloud-Native** | MigraciÃ³n del stack a servicios administrados (**AWS S3 / GCS / Dataproc / MSK**, etc.) con infraestructura como cÃ³digo. | â³ Futuro |
| âš™ï¸ **Pipeline tÃ©cnico (actual)** | **Etapa 1a â€“ Ingesta (Kafka â†’ Bronze)** | Streaming de transacciones y almacenamiento crudo en Delta Lake. | âœ… |
| | **Etapa 1b â€“ TransformaciÃ³n (Bronze â†’ Silver)** | Limpieza, parseo y validaciÃ³n de datos. | âœ… |
| | **Etapa 1c â€“ CuraciÃ³n (Silver â†’ Gold)** | Agregaciones, KPIs y datos listos para analÃ­tica. | âœ… (etapa actual) |

---

## ğŸ—ï¸ Arquitectura general


Cada capa cumple una funciÃ³n clara:
| Capa | Rol principal |
|------|----------------|
| **Bronze** | Ingesta cruda desde Kafka (eventos transaccionales). |
| **Silver** | NormalizaciÃ³n, limpieza y validaciÃ³n de calidad de datos. |
| **Gold** | Agregaciones diarias y ranking de usuarios por volumen. |

---

## âš™ï¸ Stack tecnolÃ³gico

| Componente | DescripciÃ³n |
|-------------|-------------|
| ğŸ **Python 3.10+** | Lenguaje principal. |
| â˜• **Apache Spark 3.5.1** | Motor de procesamiento distribuido. |
| ğŸ§± **Delta Lake 3.2.0** | Formato transaccional ACID sobre archivos parquet. |
| ğŸ”„ **Apache Kafka** | Ingesta de datos en tiempo real. |
| ğŸ³ **Docker Compose** | OrquestaciÃ³n local de servicios. |
| ğŸ§° **Makefile** | AutomatizaciÃ³n de comandos y flujos. |

---

## ğŸ“‚ Estructura del proyecto

| Carpeta / Archivo | DescripciÃ³n |
|--------------------|-------------|
| `jobs/` | Scripts de procesamiento (streaming, transformaciones, gold). |
| `kafka/` | Productor y consumidor Kafka. |
| `scripts/` | Scripts utilitarios y de mantenimiento. |
| `docker/` | ConfiguraciÃ³n Docker Compose. |
| `data/` | Data lake local (excluido del repo). |
| `logs/` | Logs de ejecuciÃ³n. |
| `Makefile` | Orquestador de comandos. |
| `requirements.txt` | Dependencias de Python. |
| `README.md` | DocumentaciÃ³n del proyecto. |

---

## ğŸŒ¿ Flujo de ejecuciÃ³n (modo manual)

Cada etapa se ejecuta en una terminal separada:

1. **Iniciar Kafka y Zookeeper**
   make start-docker

2. **Stream Kafka â†’ Bronze**
make stream-bronze

3. **Stream Bronze â†’ Silver (Delta)**
make bronze-to-silver

4. **Stream Silver â†’ Gold (Delta)**
make silver-to-gold

5. **Productor de transacciones falsas**
make run-producer

Logs generados:

logs/dev/
â”œâ”€ producer.log
â”œâ”€ bronze.log
â”œâ”€ silver.log
â””â”€ gold.log

---


ğŸ§° Flujo automÃ¡tico (tmux)

Si tenÃ©s tmux instalado, podÃ©s levantar todo en una sola terminal:
make tmux-up

â€¢ Detach (dejar corriendo en background): Ctrl + b, luego d

â€¢ Reanudar la sesiÃ³n:
tmux attach -t finpipe

---

ğŸ§½ Limpieza y mantenimiento

â€¢ Reiniciar entorno de desarrollo:
make reset

â€¢ Borrar logs antiguos:
make clean-logs

â€¢ Apagar todos los procesos (Spark + producer):
make kill-all

---

ğŸ§  Conceptos clave aplicados

âœ… Kafka Topics â†’ transmisiÃ³n de eventos financieros simulados.

âœ… Spark Structured Streaming â†’ lectura en tiempo real con tolerancia a fallas.

âœ… Delta Lake â†’ ACID + control de versiones + schema evolution.

âœ… Data Validation Layer â†’ separaciÃ³n de vÃ¡lidos y rechazados.

âœ… Watermarks & Deduplication â†’ manejo de tardÃ­os y duplicados.

âœ… Aggregation Layer (Gold) â†’ mÃ©tricas por fecha, usuario y moneda.

âœ… Makefile Orchestration â†’ ejecuciÃ³n reproducible del pipeline.

---

## ğŸŒ ConfiguraciÃ³n de entornos

El pipeline utiliza la variable `ENV` para alternar entre entornos configurables:

| Variable  | DescripciÃ³n |
|-----------|--------------|
| `ENV=dev` | Modo de desarrollo *(por defecto)*. |
| `ENV=prod` | Modo productivo simulado. |

**Ejemplo de ejecuciÃ³n:**
ENV=prod make bronze-to-silver

Los datos se escribirÃ¡n automÃ¡ticamente en:
data/prod/...

---

ğŸ—ºï¸ PrÃ³ximos pasos (Roadmap tÃ©cnico)

| Etapa | DescripciÃ³n                                                                                               | Estado |
| ----: | --------------------------------------------------------------------------------------------------------- | :----: |
|     2 | ğŸš€ **Orquestador:** integraciÃ³n con *Apache Airflow* o *Prefect* para gestionar dependencias y SLA.       |   ğŸ”œ   |
|     3 | â˜ï¸ **Cloud Deployment:** migraciÃ³n del stack a *AWS (S3 + MSK + EMR)* o *GCP (GCS + Dataproc + Pub/Sub)*. |    â³   |
|     4 | ğŸ“Š **VisualizaciÃ³n:** creaciÃ³n de dashboards con *Tableau*, *Power BI* o *Streamlit*.                     |   ğŸ”œ   |

---

ğŸ‘¨â€ğŸ’» Autor

MatÃ­as Ezequiel Padilla Presas
ğŸ’¼ Data Engineer | Arquitecto BIM | Python | SQL | Spark | Data Pipelines & Cloud

ğŸ”— LinkedIn: https://www.linkedin.com/in/matipadilla/

ğŸ’» GitHub: https://github.com/MP-08

â€œFinPipe fue desarrollado con enfoque en la calidad de datos, escalabilidad y buenas prÃ¡cticas de ingenierÃ­a, replicando un entorno productivo real.â€