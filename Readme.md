# ğŸš€ FinPipe â€” Data Engineering Pipeline

> **FinPipe** es un proyecto de **Data Engineering end-to-end**, diseÃ±ado para simular el flujo completo de datos financieros en tiempo real.  
> Implementa una arquitectura **medallion (Bronze â†’ Silver â†’ Gold)** utilizando **Apache Kafka**, **Apache Spark Structured Streaming** y **Delta Lake**.

---

## ğŸ§  Objetivo del proyecto

El propÃ³sito de FinPipe es construir un pipeline de datos escalable y modular que permita:
- Ingestar transacciones financieras en tiempo real.
- Procesar, validar y limpiar datos con Spark.
- Persistir datos en formato **Delta Lake**.
- Generar **tablas analÃ­ticas (Gold)** con agregaciones y rankings de usuarios.

Este pipeline emula el trabajo diario de un **Data Engineer** en un entorno productivo.

---

## ğŸ—ºï¸ Roadmap del Proyecto

### ğŸ”„ RelaciÃ³n entre los dos roadmaps: infraestructura + pipeline tÃ©cnico

| Tipo de Roadmap | Etapa | DescripciÃ³n real | Estado actual |
|-----------------|--------|------------------|---------------|
| ğŸ§± **Infraestructura (publicado)** | **Etapa 1 â€“ On-Premise / Local** | ConstrucciÃ³n del entorno local distribuido (Docker, Linux, Kafka, Spark, Delta Lake). Incluye el desarrollo del pipeline y su maduraciÃ³n (Bronze â†’ Silver â†’ Gold). | âœ… En curso |
| | **Etapa 2 â€“ Orquestador (Airflow)** | IntegraciÃ³n de **Apache Airflow** para controlar dependencias y programar los jobs Spark/Kafka. | ğŸ”œ PrÃ³ximo hito |
| | **Etapa 3 â€“ Cloud-Native** | MigraciÃ³n del stack a servicios administrados (**AWS S3 / GCS / Dataproc / MSK**, etc.) con infraestructura como cÃ³digo. | â³ Futuro |
| âš™ï¸ **Pipeline tÃ©cnico (actual)** | **Etapa 1a â€“ Ingesta (Kafka â†’ Bronze)** | Streaming de transacciones y almacenamiento crudo en Delta Lake. | âœ… |
| | **Etapa 1b â€“ TransformaciÃ³n (Bronze â†’ Silver)** | Limpieza, parseo y validaciÃ³n de datos. | âœ… |
| | **Etapa 1c â€“ CuraciÃ³n (Silver â†’ Gold)** | Agregaciones, KPIs y datos listos para analÃ­tica. | âœ… (etapa actual) |

---

## ğŸ—ï¸ Arquitectura general


Cada capa cumple una funciÃ³n clara:
| Capa | Rol principal |
|------|----------------|
| **Bronze** | Ingesta cruda desde Kafka (eventos transaccionales). |
| **Silver** | NormalizaciÃ³n, limpieza y validaciÃ³n de calidad de datos. |
| **Gold** | Agregaciones diarias y ranking de usuarios por volumen. |

---

## âš™ï¸ Stack tecnolÃ³gico

| Componente | DescripciÃ³n |
|-------------|-------------|
| ğŸ **Python 3.10+** | Lenguaje principal. |
| â˜• **Apache Spark 3.5.1** | Motor de procesamiento distribuido. |
| ğŸ§± **Delta Lake 3.2.0** | Formato transaccional ACID sobre archivos parquet. |
| ğŸ”„ **Apache Kafka** | Ingesta de datos en tiempo real. |
| ğŸ³ **Docker Compose** | OrquestaciÃ³n local de servicios. |
| ğŸ§° **Makefile** | AutomatizaciÃ³n de comandos y flujos. |

---

## ğŸ“‚ Estructura del proyecto


P1_FINPIPE/
â”œâ”€â”€ jobs/
â”‚ â”œâ”€â”€ streaming_to_bronze.py
â”‚ â”œâ”€â”€ bronze_to_silver.py
â”‚ â””â”€â”€ silver_to_gold.py
â”œâ”€â”€ kafka/
â”‚ â”œâ”€â”€ producer.py
â”‚ â””â”€â”€ test_consumer.py
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ reset_dev_delta.sh
â”‚ â””â”€â”€ ...
â”œâ”€â”€ docker/
â”‚ â””â”€â”€ docker-compose.yml
â”œâ”€â”€ data/
â”‚ â””â”€â”€ dev/ (datasets locales, excluidos del repo)
â”œâ”€â”€ logs/
â”‚ â””â”€â”€ dev/ (salidas de logs)
â”œâ”€â”€ Makefile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## ğŸ§© Flujo de ejecuciÃ³n (modo manual)

Cada etapa se ejecuta en una terminal separada ğŸ‘‡

```bash
# 1ï¸âƒ£ Iniciar Kafka y Zookeeper
make start-docker

# 2ï¸âƒ£ Iniciar el stream Kafka â†’ Bronze
make stream-bronze

# 3ï¸âƒ£ Iniciar el stream Bronze â†’ Silver (Delta)
make bronze-to-silver

# 4ï¸âƒ£ Iniciar el stream Silver â†’ Gold (Delta)
make silver-to-gold

# 5ï¸âƒ£ Largar el productor de transacciones falsas
make run-producer

---

## ğŸ§© Flujo de ejecuciÃ³n (modo manual)

Cada etapa se ejecuta en una terminal separada ğŸ‘‡

```bash
# 1ï¸âƒ£ Iniciar Kafka y Zookeeper
make start-docker

# 2ï¸âƒ£ Iniciar el stream Kafka â†’ Bronze
make stream-bronze

# 3ï¸âƒ£ Iniciar el stream Bronze â†’ Silver (Delta)
make bronze-to-silver

# 4ï¸âƒ£ Iniciar el stream Silver â†’ Gold (Delta)
make silver-to-gold

# 5ï¸âƒ£ Largar el productor de transacciones falsas
make run-producer

ğŸ’¡ Todos los logs se guardan automÃ¡ticamente en:

logs/dev/
 â”œâ”€â”€ producer.log
 â”œâ”€â”€ bronze.log
 â”œâ”€â”€ silver.log
 â””â”€â”€ gold.log

---

âš™ï¸ Flujo automÃ¡tico (modo tmux)

Si tenÃ©s instalado tmux, podÃ©s correr todo el pipeline en una sola terminal:

make tmux-up

Esto crea una sesiÃ³n con 4 paneles:

Producer

Kafka â†’ Bronze

Bronze â†’ Silver

Silver â†’ Gold

Para salir sin detener nada:

Ctrl + b  luego  d

Y para volver:

tmux attach -t finpipe

---

ğŸ§¹ Limpieza y mantenimiento

Reiniciar entorno de desarrollo:

make reset

Borrar logs antiguos:

make clean-logs

Apagar todo:

make kill-all

---

ğŸ§  Conceptos clave aplicados

âœ… Kafka Topics â†’ transmisiÃ³n de eventos financieros simulados.
âœ… Spark Structured Streaming â†’ lectura en tiempo real con tolerancia a fallas.
âœ… Delta Lake â†’ formato ACID con control de versiones y schema evolution.
âœ… Data Validation Layer â†’ separaciÃ³n automÃ¡tica de datos vÃ¡lidos y rechazados.
âœ… Watermarks & Deduplication â†’ manejo de eventos duplicados o tardÃ­os.
âœ… Aggregation Layer â†’ tablas Gold con mÃ©tricas por fecha, usuario y moneda.
âœ… Makefile Orchestration â†’ ejecuciÃ³n reproducible y controlada del pipeline.

---

ğŸŒ ConfiguraciÃ³n de entornos

El pipeline soporta mÃºltiples entornos:

ENV=dev   # por defecto
ENV=prod  # para entorno productivo simulado

Ejemplo:

ENV=prod make bronze-to-silver

Los datos se escribirÃ¡n en data/prod/....

---

ğŸ“ˆ PrÃ³ximos pasos

ğŸš€ Etapa 2 â€“ Orquestador: integraciÃ³n con Apache Airflow o Prefect.
ğŸ“¦ Etapa 3 â€“ Cloud Deployment: migraciÃ³n a AWS (S3 + MSK + EMR) o GCP (GCS + Dataproc + Pub/Sub).
ğŸ“Š Etapa 4 â€“ VisualizaciÃ³n: anÃ¡lisis y dashboards con Tableau / Power BI / Streamlit.

---

ğŸ‘¨â€ğŸ’» Autor

MatÃ­as Ezequiel Padilla Presas
ğŸ“ Data Engineer | Python, SQL, Spark | Data Pipelines & Cloud | Arquitecto BIM
ğŸ”— linkedin.com/in/matias-padilla-presas

ğŸ“¦ github.com/MP-08

---

ğŸ§  "FinPipe fue desarrollado con enfoque en la calidad de datos, escalabilidad y buenas prÃ¡cticas de ingenierÃ­a, replicando un entorno productivo real."