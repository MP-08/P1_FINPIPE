# üöÄ FinPipe ‚Äî Data Engineering Pipeline

> **FinPipe** es un proyecto de **Data Engineering end-to-end**, dise√±ado para simular el flujo completo de datos financieros en tiempo real.  
> Implementa una arquitectura **medallion (Bronze ‚Üí Silver ‚Üí Gold)** utilizando **Apache Kafka**, **Apache Spark Structured Streaming** y **Delta Lake**.

---

## üß† Objetivo del proyecto

El prop√≥sito de FinPipe es construir un pipeline de datos escalable y modular que permita:
- Ingestar transacciones financieras en tiempo real.
- Procesar, validar y limpiar datos con Spark.
- Persistir datos en formato **Delta Lake**.
- Generar **tablas anal√≠ticas (Gold)** con agregaciones y rankings de usuarios.

Este pipeline emula el trabajo diario de un **Data Engineer** en un entorno productivo.

---

## üó∫Ô∏è Roadmap del Proyecto

### üîÑ Relaci√≥n entre los dos roadmaps: infraestructura + pipeline t√©cnico

| Tipo de Roadmap | Etapa | Descripci√≥n real | Estado actual |
|-----------------|--------|------------------|---------------|
| üß± **Infraestructura (publicado)** | **Etapa 1 ‚Äì On-Premise / Local** | Construcci√≥n del entorno local distribuido (Docker, Linux, Kafka, Spark, Delta Lake). Incluye el desarrollo del pipeline y su maduraci√≥n (Bronze ‚Üí Silver ‚Üí Gold). | ‚úÖ En curso |
| | **Etapa 2 ‚Äì Orquestador (Airflow)** | Integraci√≥n de **Apache Airflow** para controlar dependencias y programar los jobs Spark/Kafka. | üîú Pr√≥ximo hito |
| | **Etapa 3 ‚Äì Cloud-Native** | Migraci√≥n del stack a servicios administrados (**AWS S3 / GCS / Dataproc / MSK**, etc.) con infraestructura como c√≥digo. | ‚è≥ Futuro |
| ‚öôÔ∏è **Pipeline t√©cnico (actual)** | **Etapa 1a ‚Äì Ingesta (Kafka ‚Üí Bronze)** | Streaming de transacciones y almacenamiento crudo en Delta Lake. | ‚úÖ |
| | **Etapa 1b ‚Äì Transformaci√≥n (Bronze ‚Üí Silver)** | Limpieza, parseo y validaci√≥n de datos. | ‚úÖ |
| | **Etapa 1c ‚Äì Curaci√≥n (Silver ‚Üí Gold)** | Agregaciones, KPIs y datos listos para anal√≠tica. | ‚úÖ (etapa actual) |

---

## üèóÔ∏è Arquitectura general


Cada capa cumple una funci√≥n clara:
| Capa | Rol principal |
|------|----------------|
| **Bronze** | Ingesta cruda desde Kafka (eventos transaccionales). |
| **Silver** | Normalizaci√≥n, limpieza y validaci√≥n de calidad de datos. |
| **Gold** | Agregaciones diarias y ranking de usuarios por volumen. |

---

## ‚öôÔ∏è Stack tecnol√≥gico

| Componente | Descripci√≥n |
|-------------|-------------|
| üêç **Python 3.10+** | Lenguaje principal. |
| ‚òï **Apache Spark 3.5.1** | Motor de procesamiento distribuido. |
| üß± **Delta Lake 3.2.0** | Formato transaccional ACID sobre archivos parquet. |
| üîÑ **Apache Kafka** | Ingesta de datos en tiempo real. |
| üê≥ **Docker Compose** | Orquestaci√≥n local de servicios. |
| üß∞ **Makefile** | Automatizaci√≥n de comandos y flujos. |

---

## üìÇ Estructura del proyecto

| Carpeta / Archivo | Descripci√≥n |
|--------------------|-------------|
| `jobs/` | Scripts de procesamiento (streaming, transformaciones, gold). |
| `kafka/` | Productor y consumidor Kafka. |
| `scripts/` | Scripts utilitarios y de mantenimiento. |
| `docker/` | Configuraci√≥n Docker Compose. |
| `data/` | Data lake local (excluido del repo). |
| `logs/` | Logs de ejecuci√≥n. |
| `Makefile` | Orquestador de comandos. |
| `requirements.txt` | Dependencias de Python. |
| `README.md` | Documentaci√≥n del proyecto. |

---

## üåø Flujo de ejecuci√≥n (modo manual)

Cada etapa se ejecuta en una terminal separada:

1. **Iniciar Kafka y Zookeeper**
   make start-docker

2. **Stream Kafka ‚Üí Bronze**
make stream-bronze

3. **Stream Bronze ‚Üí Silver (Delta)**
make bronze-to-silver

4. **Stream Silver ‚Üí Gold (Delta)**
make silver-to-gold

5. **Productor de transacciones falsas**
make run-producer

Logs generados:

logs/dev/
‚îú‚îÄ producer.log
‚îú‚îÄ bronze.log
‚îú‚îÄ silver.log
‚îî‚îÄ gold.log

---


üß∞ Flujo autom√°tico (tmux)

Si ten√©s tmux instalado, pod√©s levantar todo en una sola terminal:
make tmux-up

‚Ä¢ Detach (dejar corriendo en background): Ctrl + b, luego d

‚Ä¢ Reanudar la sesi√≥n:
tmux attach -t finpipe

---

üßΩ Limpieza y mantenimiento

‚Ä¢ Reiniciar entorno de desarrollo:
make reset

‚Ä¢ Borrar logs antiguos:
make clean-logs

‚Ä¢ Apagar todos los procesos (Spark + producer):
make kill-all

---

üß† Conceptos clave aplicados

‚úÖ Kafka Topics ‚Üí transmisi√≥n de eventos financieros simulados.

‚úÖ Spark Structured Streaming ‚Üí lectura en tiempo real con tolerancia a fallas.

‚úÖ Delta Lake ‚Üí ACID + control de versiones + schema evolution.

‚úÖ Data Validation Layer ‚Üí separaci√≥n de v√°lidos y rechazados.

‚úÖ Watermarks & Deduplication ‚Üí manejo de tard√≠os y duplicados.

‚úÖ Aggregation Layer (Gold) ‚Üí m√©tricas por fecha, usuario y moneda.

‚úÖ Makefile Orchestration ‚Üí ejecuci√≥n reproducible del pipeline.

---

## üåç Configuraci√≥n de entornos

El pipeline utiliza la variable `ENV` para alternar entre entornos configurables:

| Variable  | Descripci√≥n |
|-----------|--------------|
| `ENV=dev` | Modo de desarrollo *(por defecto)*. |
| `ENV=prod` | Modo productivo simulado. |

**Ejemplo de ejecuci√≥n:**
ENV=prod make bronze-to-silver

Los datos se escribir√°n autom√°ticamente en:
data/prod/...

---

üó∫Ô∏è Pr√≥ximos pasos (Roadmap t√©cnico)

| Etapa | Descripci√≥n                                                                                               | Estado |
| ----: | --------------------------------------------------------------------------------------------------------- | :----: |
|     2 | üöÄ **Orquestador:** integraci√≥n con *Apache Airflow* o *Prefect* para gestionar dependencias y SLA.       |   üîú   |
|     3 | ‚òÅÔ∏è **Cloud Deployment:** migraci√≥n del stack a *AWS (S3 + MSK + EMR)* o *GCP (GCS + Dataproc + Pub/Sub)*. |    ‚è≥   |
|     4 | üìä **Visualizaci√≥n:** creaci√≥n de dashboards con *Tableau*, *Power BI* o *Streamlit*.                     |   üîú   |

---

## üë®‚Äçüíª Autor

**Mat√≠as Ezequiel Padilla Presas**  
üíº *Data Engineer | Arquitecto BIM | Python | SQL | Spark | Data Pipelines & Cloud*  

üìé **LinkedIn:** [linkedin.com/in/matias-padilla-presas](https://linkedin.com/in/matias-padilla-presas)  
üíª **GitHub:** [github.com/MP-08](https://github.com/MP-08)

> *FinPipe fue desarrollado con enfoque en la calidad de datos, escalabilidad y buenas pr√°cticas de ingenier√≠a, replicando un entorno productivo real.*
